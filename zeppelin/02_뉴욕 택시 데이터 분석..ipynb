{
  "metadata": {
    "name": "02_뉴욕 택시 데이터 분석",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql import SparkSession\n\nspark \u003d SparkSession.builder.appName(\"trip_count_sql\").getOrCreate()\nspark"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#데이터가 들어 있는 디렉토리 지정\ndirectory \u003d \"/home/ubuntu/working/datasource\"\n\ntrips_files \u003d \u0027trips/*\u0027\nzone_file \u003d \u0027taxi+_zone_lookup.csv\u0027"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntrips_df \u003d spark.read.csv(f\"file://{directory}/{trips_files}\", inferSchema \u003d True, header \u003d True)\nzone_df \u003d spark.read.csv(f\"file://{directory}/{zone_file}\", inferSchema \u003d True, header \u003d True)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntrips_df.printSchema()\nzone_df.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Datalake -\u003e Data Warehoused\n\n- 필요한 데이터"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntrips_df.createOrReplaceTempView(\u0027trips\u0027)\nzone_df.createOrReplaceTempView(\u0027zone\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nquery \u003d \u0027\u0027\u0027\nSELECT * FROM trips\n\u0027\u0027\u0027\n\nz.show(spark.sql(query))"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nquery \u003d \u0027\u0027\u0027\nSELECT * FROM zone\n\u0027\u0027\u0027\n\nz.show(spark.sql(query))\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Warehouse\nquery \u003d \"\"\"\n    SELECT\n        t.VendorID as vendor_id,\n        TO_DATE(t.tpep_pickup_datetime) as pickup_date,\n        TO_DATE(t.tpep_dropoff_datetime) as dropoff_date,\n        HOUR(t.tpep_pickup_datetime) as pickup_time,\n        HOUR(t.tpep_dropoff_datetime) as dropoff_time,\n        \n        t.passenger_count,\n        t.trip_distance,\n        t.fare_amount,\n        t.tip_amount,\n        t.tolls_amount,\n        t.total_amount,\n        t.payment_type,\n        \n        pz.Zone as pickup_zone,\n        dz.Zone as dropoff_zone\n    FROM trips t\n    \n    LEFT JOIN zone pz ON t.PULocationID \u003d pz.locationID\n    LEFT JOIN zone dz ON t.DOLocationID \u003d dz.locationID\n\"\"\"\n\n# comb_df가 Warehouse의 역할\n\ncomb_df \u003d spark.sql(query)\nz.show(comb_df)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Data Warehouse -\u003e Mart\n-웨어하우스에서 마트를 만들기 위해서는 데이터를 검사, 정제\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ncomb_df.createOrReplaceTempView(\"comb\")"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 1. 날짜와 시간을 검사\nquery \u003d \u0027\u0027\u0027\nSELECT\n    pickup_date,\n    pickup_time\nFROM comb\nWHERE pickup_time \u003e\u003d 0\nORDER BY pickup_date\n\u0027\u0027\u0027\n\nz.show(spark.sql(query))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "-- %sql\n\n-- # 1. 날짜와 시간을 검사\n\n-- SELECT\n-- FROM comb\n-- WHERE pickup_time \u003e\u003d 0\n-- ORDER BY pickup_date\n\n-- z.show(spark.sql(query))"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nquery \u003d \u0027\u0027\u0027 \nSELECT count(*)\nFROM comb\nWHERE pickup_date \u003c \u00272021-01-01\u0027\n\u0027\u0027\u0027\nz.show(spark.sql(query))"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 2. 요금 데이터 확인\n\ncomb_df_describe \u003d comb_df.select(\u0027total_amount\u0027).describe()\nz.show(comb_df_describe)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 3. 거리 데이터 확인\ncomb_df.distance \u003d comb_df.select(\u0027trip_distance\u0027).describe()\nz.show(comb_df.distance)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n# 4. 월 별 운행 수 확인\r\nquery \u003d \u0027\u0027\u0027\r\nSELECT\r\n    DATE_TRUNC(\u0027MM\u0027, pickup_date) as month,\r\n    COUNT(*) as trips\r\nFROM comb\r\nGROUP BY month\r\nORDER BY month DESC\r\n\u0027\u0027\u0027\r\n\r\nz.show(spark.sql(query))\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 5. 승객에 대한 통계정보 확인\ncomb_df_passenger_cnt \u003d comb_df.select(\u0027passenger_count\u0027).describe()\nz.show(comb_df_passenger_cnt)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 살펴본 내용을 토대로 실제 분석할 데이터로 정제. WareHoues \u003d\u003e Mart"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 데이터 정제\n\nquery \u003d \u0027\u0027\u0027\nSELECT *\nFROM comb c\nWHERE c.total_amount \u003c 200\n    AND c.total_amount \u003e 0\n    \n    AND c.passenger_count \u003c 5\n    AND c.pickup_date \u003e\u003d \u00272021-01-01\u0027\n    \n    AND c.trip_distance \u003c 10\n    AND c.trip_distance \u003e 0\n\u0027\u0027\u0027\n\n# cleaned_df가 mart의 역활을 한다. 즉 데이터 분석할 상태가 되었다는것을 의미\ncleaned_df \u003d spark.sql(query)\ncleaned_df.createOrReplaceTempView(\u0027cleaned\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nz.show(cleaned_df.describe())"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%MD\nEDA\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# pickup date 별 운행 수 확인\n\nquery \u003d \u0027\u0027\u0027\nSELECT\n    pickUP_date,\n    COUNT(pickup_date) as trips\nFROM cleaned\nGROUP BY pickup_date\nORDER BY pickup_date ASC\n\u0027\u0027\u0027\n\nz.show(spark.sql(query))"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 요일 별 운행 수 확인\n\nquery \u003d \u0027\u0027\u0027\nSELECT\n    DATE_TRUNC(\u0027MM\u0027, pickup_date) as month,\n    DATE_FORMAT(pickup_date, \u0027EEEE\u0027) as day_of_week,\n    COUNT(*) as trips\nFROM cleaned\nGROUP BY month, day_of_week\nORDER BY month, day_of_week\n\u0027\u0027\u0027\n\nz.show(spark.sql(query))"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 결제 유형 분석\n\nz.show(cleaned_df.select(\u0027payment_type\u0027))"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 숫자로 되어 있는 결제 타입을 문자열로 변경하기\ndef parse_payment_type(payment_type):\n\n    payment_type_to_string \u003d {\n      1: \"Credit Card\",\n      2: \"Cash\",\n      3: \"No Charge\",\n      4: \"Dispute\",\n      5: \"Unknown\",\n      6: \"Voided Trip\",\n    }\n\n    return payment_type_to_string[payment_type]"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark.udf.register(\u0027parse_payment_type\u0027, parse_payment_type)"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 결제 타입 별 통계\n\nquery \u003d \u0027\u0027\u0027\nSELECT\n    parse_payment_type(payment_type) as payment_type_str,\n    COUNT(*) as trips,\n    MEAN(fare_amount) as mean_fare_amount,\n    STD(fare_amount) as std_fare_amount\nFROM cleaned\nGROUP BY payment_type\n\u0027\u0027\u0027\n\nz.show(spark.sql(query))"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark.stop()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}